\chapter{Discussion and Conclusion}\label{chapter:conclusion}
\thispagestyle{chapterBeginStyle}

\vspace{-15mm}
The discussion in this chapter focuses separately on each part of the experiments. This division is due to other aspects of the problem being touched. The first part, named \textit{"Don't Augment Me"}, focuses on the problem with XAI methods, when the second part, titled \textit{"Can I Rely On You"}, discusses a broader problem of being able to compare these methods. The chapter ends with the conclusion and presents the idea of possible future exploration in the field.

\section{Don't Augment Me}

The common practice in designing machine learning models is to use predefined building blocks, and with the evolution of frameworks, it is going to become even more accessible for non-researchers to develop and deploy models. Because explainability is becoming a requirement, some of those developers are going to use available XAI methods base on their popularity. The usual process of understanding the models' decision is to check the input attribution for a set of test examples, and base on that attribution, decide if the model is working correctly. As shown in this thesis, that kind of approach can provide us with insufficient information to understand our model. Real-world examples might provide a different explanation than the ones from the test set (see Figure \ref{fig:augmentation-example} and the \textit{cardigan} attributions for original and locally normalized images).

\vspace{\baselineskip}

The first experiment results gave us an understanding of how the current XAI methods behave when the input of the model is augmented with easily accessible augmentations. Base on similarity comparison, we could deduct that the method with the highest robustness is Guided GradCAM. This method achieved the highest SSIM scores with the lowest standard deviation, but like mentioned in section \ref{results:dont-augment-me}, the GradCAM method is far from being ideal (see Fig. \ref{fig:xai-tiramisu}). Every method tested in this study appears to be influenced by the perturbations that have little to no effect on the models' prediction. This is concerning because the methods' behavior is nondeterministic. When applied to the actual model, we cannot be sure if the result given by the method is the correct one unless we test every single one.

\vspace{\baselineskip}

The positive aspect of the experiments is that the robustness of the method is changing in a minimal way between different types of augmentations. The substantial change is seen only in the case of Deconvolution (see Fig, \ref{fig:SSIM-mean-rotation}), where the method has a significant drop in the performance. The drop was caused by the way Deconvolution presents the attributions and the difference between the attribution of the rotated image and the rotated attribution of the original image. This issue could be fixed by using a different methodology than initially assumed. However, the SSIM metric is not an ideal metric and can be improved to match the human visual perception better. Even with those flaws, the results were consistent across different methods and augmentations.

\section{Can I Rely On You}

Base on results from the first part of the experiments, we can agree that having a reliable and comparable measure is beneficial. With that measure, we could compare existing XAI methods and improve new ones. Two of the most popular XAI methods were tested in the second part of the experiments. The results differed between the methods, and it is better to discuss them separately. 

\vspace{\baselineskip}

Infidelity was the first measure tested against multiple models and datasets. The results showed that this measure is extremely dependent on the dataset, model architecture, and even model performance. It failed the reliability test on any level, and the results showed that values returned by the measure could not be compared even within the same trained model. Even if it has a strong, sensible, theoretical background, the absolute value of infidelity cannot be used to compare the two values. The relative value of infidelity for method A when compared with method B changes within the same model architecture and dataset just after the model performance changes (see Fig. \ref{fig:inf-metrics-examples}). A method that was considered the best (lowest infidelity score) after the model is better trained on the same dataset achieves the worst result. That kind of behavior is unacceptable in the measure. Even values of the infidelity from the same trained model and methods are inconsistent (see a comparison of infidelity values between Fig. \ref{fig:resnet-inf-96} and Fig. \ref{fig:resnet-inf-107}.

\vspace{\baselineskip}

Sensitivity measure achieves far better results than infidelity, but unlike infidelity, it is not a measure of quality but the measure of robustness. As shown in Figures \ref{fig:resnet-sens} and \ref{fig:sens-metrics-examples}, the measure is able to compare different methods within the same model architecture, and there is less value change between different accuracy. However, the absolute value of sensitivity is still a meaningless value because the value changes between architectures. The mean value for the ResNet18 cannot be compared with the value for EfficientNet B0 (compare Fig. \ref{fig:resnet-sens} and Fig. \ref{fig:efficientnet-sens}). The order of methods also differs between different architectures. Base on that information, we can assume that sensitivity could be used as a measure within the same architecture, and because ML practitioners are usually reusing a limited set of predefined architectures, there could be a set of XAI methods that work best for a specific architecture. During the experiments, the sensitivity seems to give the lower scores to the methods with less detailed attributions (see Fig. \ref{fig:sens-scores-examples}). Attribution methods that work more like edge detectors (GBP or GradCAM) tend to receive larger values of sensitivity.

\section{Conclusion}

XAI is still a new branch of the field but already gotten implemented in some of the popular libraries \cite{kokhlikyan2020captum, tensorflow2015-whitepaper}, and even by a top-tier cloud provider \cite{gcp_ig}. Package developers and cloud service providers are doing their best to help the engineers understand their models, but as shown in the number of papers and this thesis, current methods are far from being a ready solution. Without understanding the particular method's drawbacks, the method might provide a false sense of understanding for the developer and cause a severe problem for the end-user.

\vspace{\baselineskip}

As shown in this study, the reliability and robustness of XAI methods are a major issue. Not having a sensible measure to compare those methods is one of the concerns because we as a community are not able to validate if a newly released method is better than the previous one. This also does not help the authors of the methods because they have to rely on qualitative measures, like manual attribution comparison, to determine the quality of their work. This could cause a problem with cherry-picked examples presented in scientific papers. To provide the confirmation that our new method works, we only have to find a handful of examples to include in a paper instead of calculating a numerical value that determines the quality of the method.

\section{Future Work}

This thesis focused mostly on exposing the problem with XAI methods and available measures. With the results, new ideas for improving the current state of the field emerged. One of those ideas is to create a reliable measure for XAI methods which is based on structural similarities of the attribution for visual tasks. SSIM used in this thesis is just a basic structural similarity metric and already have multiple improved version. This should also fix the issue with methods similar to Deconvolution, which had terrible scores on rotated images. Another idea is to improve the measure of infidelity to be able to compare not only the reliability of the method but the quality as well. Eventually, the goal is to create a set of tests integrated with one of the popular XAI frameworks, which can be performed on any method to determine its quality and robustness.