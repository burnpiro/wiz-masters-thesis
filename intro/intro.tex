\chapter{Introduction}\label{chapter:intro}
\thispagestyle{chapterBeginStyle}

In recent years, the field of machine learning (ML) has reached a wider audience with the introduction of digital assistants, self-driving cars, and many other innovations. This interest was caused by making machine learning models more accessible and easy to train. The increase in processing power allowed to train larger and better deep neural networks (DNN), which reached a point when they can outperform humans under certain conditions \cite{dodge2017study, sturman2020deep, rank2020deep, wani2020advances}. Unfortunately, while drifting from the classic rule-based systems, we have lost the notion of explainability. The current models can reach up to trillion parameters \cite{fedus2021switch} which are far beyond what a human can comprehend. These models are referred to as black-boxes, and they are not providing insides to how the decision was made. They are in opposition to more transparent techniques like decision trees or association rules, where the decision process is usually much clearer, but the accuracy is lower. With this side of the model complexity and the issue of understanding the decision, people tend not to trust the models when they do not provide the explanation \cite{edwards2017slave}.

\section{Explainable Artificial Intelligence}

Explainable Artificial Intelligence (XAI) is one of the youngest and one of the fastest developing branches of the field. The objective of the XAI method is to provide an explanation for the deep learning model that is understandable by humans. This is especially important in safety-critical domains like healthcare or security. The methods presented in the literature over the years often promise that they will provide a clear answer to the question of how the model made its decision. With that promise and relative ease of use (most of the popular methods are available in popular XAI libraries), they are often used by non-professionals when designing the models. Authors of the most popular XAI methods are releasing the papers with a specially selected set of examples, which do not cover a wide range of situations where a specific method can be used.

\vspace{\baselineskip}

XAI methods have been used for explaining models used in healthcare \cite{rethmeier2020efficare, sidhom2021deep, kleppe2021designing, xiao2021screening, van2020systematic, mamandipoor2021machine, lind2021artificial}, genes analysis \cite{liu2021interrogation}, hate speech detection \cite{kovacs2021challenges}, drug discovery \cite{jimenez2021artificial}, Covid-19 X-Ray analysis \cite{panwar2020deep, ahmed2021discovery} and fraud detection \cite{sinanc2021explainable}, Parkinsonâ€™s Disease recognition \cite{pianpanit2021parkinson}, and other areas \cite{kovacs2021challenges, asif2020deepselex}.

\vspace{\baselineskip}

The range of measures that can be used to compare XAI methods is very limited. Only two measures called \textit{Infidelity} and \textit{Sensitivity}\cite{yeh2019fidelity} were implemented in the most popular XAI library \cite{kokhlikyan2020captum}. Those measures have a solid theoretical background and are used to compare which XAI method works better. 

\section{Goal of the study}

This thesis is going to investigate XAI methods in terms of their robustness to real-world augmentation of the input data. Additionally, the measures used to compare these methods are also part of the experiments. The goal is to verify if the measure can be used to compare the quality of XAI methods. The study is focused on the interpretability of image-recognition tasks and the attribution of the Convolutional Neural Networks (CNNs) \cite{lecun1995convolutional, lecun1989backpropagation}.

\vspace{\baselineskip}

Methods tested in this study are limited to attribution methods. They were selected base on popularity and availability in popular XAI frameworks with the assumption that the most available methods have the biggest impact on the field of machine learning. To check the effect of applying augmentations on the XAI methods, the measure of similarity is going to be used. This measure is going to check how much the attribution provided by the method changes if the augmentation does not affect the models' performance.To test the measures, a set of additional experiments is going to be performed. This should help to decide wherever the measure is reliable to compare two XAI methods.

\vspace{\baselineskip}

At the end of the study, we should be able to answer which XAI methods are the most robust and what their limitations are. We also should be able to explain the effect of real-world augmentation on the performance of such methods as well as describe the reason why some methods perform better than others. Furthermore, we should have an understanding of the reliability of the measures used to compare XAI methods and if they should be used in practice to compare methods. If yes, then what are the constraints, and how to interpret the results of such comparison.

\section{Outline}

In the following chapter, the theoretical background for this study is going to be presented. The chapter focuses on exploring the idea of machine learning and the backpropagation method, as well as introducing the idea of CNNs. It also contains the definitions of interpretability and explainability, and a brief description of the idea of attribution methods and types of measures used for research. The third chapter contains a detailed explanation of every XAI method used in the study and the additional technique used to improve the quality of the attribution methods. Next, in chapter \ref{chapter:measures}, the measures of XAI methods are presented: main two measures that are the subject of experiments, an additional measure used to compare similarities of the attribution, and one measure of XAI methods from the literature that is worth discussing but because of its computational complexity, cannot be reproduced easily. The fifth chapter is a description of the experiments performed in this thesis. Experiments are divided into two parts. The first part is related to the robustness of the XAI methods and is called \textit{"Don\'t Augment Me"} (see section \ref{section:dont-augment-me-definition}). The second part is focused on the measures used to compare XAI methods and their reliability, and this part is called \textit{"Can I Rely On You?"} (see section \ref{section:can-i-rely-on-you-definition}). To execute these experiments, the preparation of the environment is required. That is why the first part of chapter 5 contains a description of the datasets, training procedure for models, and the description of the augmentations used in the experiments. Finally, chapter \ref{chapter:results} contains all the results of the experiments. The thesis is closed with chapter \ref{chapter:conclusion}, where this study and its effects are discussed. In addition to the thesis, appendix section \ref{appendix:osf} contains instructions on how to reproduce the experiments. Supplementary materials are also in the appendix (section \ref{appendix:supplementary}), including additional charts and detailed results.