\section*{Abstract}

In recent years, the field of Explainable Artificial Intelligence (XAI) has become increasingly important. Many new approaches to the interpretability of the deep learning models have been proposed. However, with the rise of new methods, it had become even harder to compare them and decide which one should be used for a particular solution. This thesis focuses on two important aspects of interpretability: robustness of the methods and reliability of the measures used to compare these methods. The first part checks the effect of real-world augmentations on the attribution methods by comparing similarity scores of those attributions. The results obtained during the experiments imply that even the most robust methods should not be used without a deeper understanding of potential flaws in producing the explanation. The second part concentrates on analyzing the popular measures and checks if they are reliable and ready to use for machine learning practitioners. The outcome from the experiments indicates that current measures are not reliable for comparing XAI methods. They are unable to meet the definition of a reliable measure. The measure of sensitivity can be used under some restricted conditions, but the scores returned by that measure have little value as we try to use them as a numerical measure.

\section*{Streszczenie}

W ostatnich latach, dziedzina Wyjaśnialnej Sztucznej Inteligencji (XAI) stawała się coraz bardziej istotna. Zostało zaproponowane wiele nowych podejść do interpretowalności modeli głębokich. Jednak, wraz z pojawieniem się tych nowych metod, porównywanie oraz decyzja którą metodę użyć w konkretnym rozwiązaniu stało się trudniejsze. Ta praca skupia się na dwóch istotnych aspektach interpretowalności: odporności tych metod, oraz niezawodności miar służących do porównywania wspomnianych metod. Pierwsza część sprawdza efekty jakie na metodach atrybucyjnych robią spotykane w świecie rzeczywistym augmentacje danych. Rezultaty uzyskane podczas eksperymentów sugerują, że nawet najbardziej odporne metody, nie powinny być używane bez głębszego zrozumienia potencjalnych wad występujących podczas generowania atrybucji. Druga część skupia się na analizie popularnych miar i sprawdza, czy są one niezawodne oraz gotowe do użycia przez praktyków uczenia maszynowego. Wynik eksperymentów wskazuje, że obecne miary nie są niezawodne w porównywaniu metod XAI. Nie są w stanie spełnić definicji niezawodnej miary. Miara wrażliwości (ang. sensitivity) może być używana pod warunkiem spełnienia pewnych restrykcyjnych warunków, lecz wyniki zwracane przez nią, mają małą wartość, gdybyśmy chcieli traktować je jako miarę liczbową.